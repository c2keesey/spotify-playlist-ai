{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset import SpotifyPlaylistDataset\n",
    "import torch\n",
    "from torch.utils.data import Subset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "from data_utils import print_playlist_track_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = '../_data/main_set.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Playlist Track Counts\n",
      "--------------------\n",
      "Vantage ⏣ 🜔 ⌬: 350 tracks\n",
      "Awake ⏣ ⌬: 238 tracks\n",
      "Dissolve ⏣ ⌬: 130 tracks\n",
      "Aeon 2 ⏣ ⌬: 120 tracks\n",
      "Horizon ⏣ ⌬: 126 tracks\n",
      "Good Life ⏣ ⌬: 124 tracks\n",
      "WALK UP LIKE 🜂: 308 tracks\n",
      "Evolution 🜄: 199 tracks\n",
      "Simp DM 🜄: 152 tracks\n",
      "Glitch Mind 🜂🜁: 272 tracks\n",
      "Overclock 🜁: 109 tracks\n",
      "Critical Path 🜃: 508 tracks\n",
      "CRANK 🜍🜂: 112 tracks\n",
      "Bump Beats 🜍: 249 tracks\n",
      "Keep it smooth 🜩: 111 tracks\n",
      "🜊 Hoppin: 203 tracks\n",
      "Ain't Noise Pollution: 363 tracks\n",
      "🝓☉ Alt n shit ☉🝓: 151 tracks\n",
      "🜻 Chef Up: 118 tracks\n",
      "Reggae: 127 tracks\n",
      "☉ Pacific: 397 tracks\n",
      "Indie: 308 tracks\n",
      "Creekwalk: 310 tracks\n",
      "Chill/Solo Guitar: 207 tracks\n",
      "☿gone beyond going☿: 104 tracks\n",
      "--------------------\n",
      "Total playlists: 25\n"
     ]
    }
   ],
   "source": [
    "print_playlist_track_counts(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "dataset = SpotifyPlaylistDataset(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total samples: 5396\n",
      "Training samples: 4316\n",
      "Testing samples: 1080\n"
     ]
    }
   ],
   "source": [
    "# Get the total number of samples\n",
    "n_samples = len(dataset)\n",
    "\n",
    "# Create indices for all samples\n",
    "indices = list(range(n_samples))\n",
    "\n",
    "# Get unique playlist IDs\n",
    "playlist_ids = dataset.playlist_ids\n",
    "\n",
    "# Create the split\n",
    "train_indices, test_indices = train_test_split(\n",
    "    indices, \n",
    "    test_size=0.2,  # 20% for testing, 80% for training\n",
    "    stratify=playlist_ids,  # This ensures that the split maintains the proportion of samples for each playlist\n",
    "    random_state=42  # For reproducibility\n",
    ")\n",
    "\n",
    "# Create Subset objects\n",
    "train_dataset = Subset(dataset, train_indices)\n",
    "test_dataset = Subset(dataset, test_indices)\n",
    "\n",
    "print(f\"Total samples: {n_samples}\")\n",
    "print(f\"Training samples: {len(train_dataset)}\")\n",
    "print(f\"Testing samples: {len(test_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32  # You can adjust this based on your needs and computational resources\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Centroids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_playlist_centroids(dataset, indices):\n",
    "    playlist_features = defaultdict(list)\n",
    "    for idx in indices:\n",
    "        playlist_name = dataset.playlist_names[idx]\n",
    "        features = dataset.features[idx]\n",
    "        playlist_features[playlist_name].append(features)\n",
    "    \n",
    "    playlist_centroids = {}\n",
    "    for playlist, features in playlist_features.items():\n",
    "        playlist_centroids[playlist] = np.mean(features, axis=0)\n",
    "    \n",
    "    return playlist_centroids\n",
    "\n",
    "train_playlist_centroids = compute_playlist_centroids(dataset, train_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_song(song_features, playlist_centroids, threshold=0.9):\n",
    "    similarities = {}\n",
    "    for playlist, centroid in playlist_centroids.items():\n",
    "        similarity = cosine_similarity([song_features], [centroid])[0][0]\n",
    "        similarities[playlist] = similarity\n",
    "    \n",
    "    # Sort playlists by similarity\n",
    "    sorted_playlists = sorted(similarities.items(), key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    # Select playlists above the threshold\n",
    "    selected_playlists = []\n",
    "    max_similarity = sorted_playlists[0][1]\n",
    "    for playlist, similarity in sorted_playlists:\n",
    "        if similarity >= max_similarity * threshold:\n",
    "            selected_playlists.append(playlist)\n",
    "        else:\n",
    "            break\n",
    "    \n",
    "    return selected_playlists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.2879\n",
      "Recall: 0.4074\n",
      "F1 Score: 0.3044\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "\n",
    "def evaluate_multi_label(dataset, indices, playlist_centroids, threshold=0.9):\n",
    "    true_labels = []\n",
    "    predicted_labels = []\n",
    "    \n",
    "    for idx in indices:\n",
    "        song_features = dataset.features[idx]\n",
    "        true_playlist = dataset.playlist_names[idx]\n",
    "        predicted_playlists = classify_song(song_features, playlist_centroids, threshold)\n",
    "        \n",
    "        # Create binary vectors for true and predicted labels\n",
    "        all_playlists = list(playlist_centroids.keys())\n",
    "        true_vector = [1 if playlist == true_playlist else 0 for playlist in all_playlists]\n",
    "        pred_vector = [1 if playlist in predicted_playlists else 0 for playlist in all_playlists]\n",
    "        \n",
    "        true_labels.append(true_vector)\n",
    "        predicted_labels.append(pred_vector)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(true_labels, predicted_labels, average='weighted')\n",
    "    \n",
    "    return precision, recall, f1\n",
    "\n",
    "# Evaluate\n",
    "precision, recall, f1 = evaluate_multi_label(dataset, test_indices, train_playlist_centroids)\n",
    "\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1 Score: {f1:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Playlist Metrics:\n",
      "Playlist\t\tPrecision\tRecall\t\tF1 Score\tSupport\n",
      "Awake ⏣ ⌬           \t0.3051\t\t0.3750\t\t0.3364\t\t48\n",
      "Keep it smooth 🜩    \t0.1273\t\t0.3182\t\t0.1818\t\t22\n",
      "Bump Beats 🜍        \t0.3563\t\t0.6200\t\t0.4526\t\t50\n",
      "Horizon ⏣ ⌬         \t0.0933\t\t0.2800\t\t0.1400\t\t25\n",
      "WALK UP LIKE 🜂      \t0.3402\t\t0.5323\t\t0.4151\t\t62\n",
      "🝓☉ Alt n shit ☉🝓    \t0.1233\t\t0.3000\t\t0.1748\t\t30\n",
      "Chill/Solo Guitar   \t0.4405\t\t0.9024\t\t0.5920\t\t41\n",
      "Creekwalk           \t0.3478\t\t0.5161\t\t0.4156\t\t62\n",
      "Critical Path 🜃     \t0.5172\t\t0.1471\t\t0.2290\t\t102\n",
      "Ain't Noise Pollution\t0.4231\t\t0.3014\t\t0.3520\t\t73\n",
      "Evolution 🜄         \t0.0964\t\t0.2000\t\t0.1301\t\t40\n",
      "Vantage ⏣ 🜔 ⌬       \t0.3861\t\t0.5571\t\t0.4561\t\t70\n",
      "Glitch Mind 🜂🜁      \t0.1579\t\t0.2778\t\t0.2013\t\t54\n",
      "🜻 Chef Up           \t0.1235\t\t0.4167\t\t0.1905\t\t24\n",
      "🜊 Hoppin            \t0.2525\t\t0.6098\t\t0.3571\t\t41\n",
      "Dissolve ⏣ ⌬        \t0.1047\t\t0.3462\t\t0.1607\t\t26\n",
      "Indie               \t0.2500\t\t0.1935\t\t0.2182\t\t62\n",
      "☉ Pacific           \t0.3288\t\t0.3038\t\t0.3158\t\t79\n",
      "Reggae              \t0.2250\t\t0.7200\t\t0.3429\t\t25\n",
      "Aeon 2 ⏣ ⌬          \t0.0769\t\t0.1250\t\t0.0952\t\t24\n",
      "Simp DM 🜄           \t0.1935\t\t0.6000\t\t0.2927\t\t30\n",
      "Overclock 🜁         \t0.1867\t\t0.6364\t\t0.2887\t\t22\n",
      "Good Life ⏣ ⌬       \t0.0714\t\t0.1600\t\t0.0988\t\t25\n",
      "CRANK 🜍🜂            \t0.1667\t\t0.4545\t\t0.2439\t\t22\n",
      "☿gone beyond going☿ \t0.2632\t\t0.9524\t\t0.4124\t\t21\n"
     ]
    }
   ],
   "source": [
    "def print_playlist_metrics(dataset, indices, playlist_centroids, threshold=0.9):\n",
    "    true_labels = []\n",
    "    predicted_labels = []\n",
    "    all_playlists = list(playlist_centroids.keys())\n",
    "    \n",
    "    for idx in indices:\n",
    "        song_features = dataset.features[idx]\n",
    "        true_playlist = dataset.playlist_names[idx]\n",
    "        predicted_playlists = classify_song(song_features, playlist_centroids, threshold)\n",
    "        \n",
    "        true_vector = [1 if playlist == true_playlist else 0 for playlist in all_playlists]\n",
    "        pred_vector = [1 if playlist in predicted_playlists else 0 for playlist in all_playlists]\n",
    "        \n",
    "        true_labels.append(true_vector)\n",
    "        predicted_labels.append(pred_vector)\n",
    "    \n",
    "    precision, recall, f1, support = precision_recall_fscore_support(true_labels, predicted_labels, average=None)\n",
    "    \n",
    "    print(\"\\nPlaylist Metrics:\")\n",
    "    print(\"Playlist\\t\\tPrecision\\tRecall\\t\\tF1 Score\\tSupport\")\n",
    "    for i, playlist in enumerate(all_playlists):\n",
    "        print(f\"{playlist:<20}\\t{precision[i]:.4f}\\t\\t{recall[i]:.4f}\\t\\t{f1[i]:.4f}\\t\\t{support[i]}\")\n",
    "\n",
    "# Print detailed metrics\n",
    "print_playlist_metrics(dataset, test_indices, train_playlist_centroids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total samples: 5396\n",
      "Training samples: 4316\n",
      "Testing samples: 1080\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "import numpy as np\n",
    "from dataset import SpotifyPlaylistDataset\n",
    "from dataset import SpotifyPlaylistDataset\n",
    "from torch.utils.data import Subset, DataLoader\n",
    "\n",
    "\n",
    "\n",
    "# Assuming we still have our dataset and splits from before\n",
    "dataset = SpotifyPlaylistDataset('../_data/main_set.json')\n",
    "# ... (train_indices and test_indices creation code here)\n",
    "\n",
    "\n",
    "# Get the total number of samples\n",
    "n_samples = len(dataset)\n",
    "\n",
    "# Create indices for all samples\n",
    "indices = list(range(n_samples))\n",
    "\n",
    "# Get unique playlist IDs\n",
    "playlist_ids = dataset.playlist_ids\n",
    "\n",
    "# Create the split\n",
    "train_indices, test_indices = train_test_split(\n",
    "    indices, \n",
    "    test_size=0.2,  # 20% for testing, 80% for training\n",
    "    stratify=playlist_ids,  # This ensures that the split maintains the proportion of samples for each playlist\n",
    "    random_state=42  # For reproducibility\n",
    ")\n",
    "\n",
    "# Create Subset objects\n",
    "train_dataset = Subset(dataset, train_indices)\n",
    "test_dataset = Subset(dataset, test_indices)\n",
    "\n",
    "print(f\"Total samples: {n_samples}\")\n",
    "print(f\"Training samples: {len(train_dataset)}\")\n",
    "print(f\"Testing samples: {len(test_dataset)}\")\n",
    "\n",
    "# Get all unique playlist names\n",
    "all_playlists = list(set(dataset.playlist_names))\n",
    "\n",
    "# Create multi-label binarizer\n",
    "mlb = MultiLabelBinarizer()\n",
    "mlb.fit([all_playlists])  # Fit with all possible playlist names\n",
    "\n",
    "# Prepare data for PyTorch\n",
    "X_train = torch.FloatTensor(dataset.features[train_indices])\n",
    "y_train = torch.FloatTensor(mlb.transform([[playlist] for playlist in np.array(dataset.playlist_names)[train_indices]]))\n",
    "X_test = torch.FloatTensor(dataset.features[test_indices])\n",
    "y_test = torch.FloatTensor(mlb.transform([[playlist] for playlist in np.array(dataset.playlist_names)[test_indices]]))\n",
    "\n",
    "# Create DataLoader objects\n",
    "train_data = TensorDataset(X_train, y_train)\n",
    "test_data = TensorDataset(X_test, y_test)\n",
    "train_loader = DataLoader(train_data, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(test_data, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PlaylistClassifier(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_classes):\n",
    "        super(PlaylistClassifier, self).__init__()\n",
    "        self.layer1 = nn.Linear(input_size, hidden_size)\n",
    "        self.layer2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.layer3 = nn.Linear(hidden_size, num_classes)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.layer1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.relu(self.layer2(x))\n",
    "        x = self.dropout(x)\n",
    "        x = torch.sigmoid(self.layer3(x))\n",
    "        return x\n",
    "\n",
    "# Initialize the model\n",
    "input_size = X_train.shape[1]\n",
    "hidden_size = 64\n",
    "num_classes = len(all_playlists)\n",
    "model = PlaylistClassifier(input_size, hidden_size, num_classes)\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.BCELoss()\n",
    "# optimizer = optim.Adam(model.parameters())\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, criterion, optimizer, num_epochs=10):\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        for inputs, labels in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
    "\n",
    "def evaluate_model(model, test_loader, threshold=0.5):\n",
    "    model.eval()\n",
    "    all_predictions = []\n",
    "    all_labels = []\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            outputs = model(inputs)\n",
    "            predictions = (outputs >= threshold).float()\n",
    "            all_predictions.extend(predictions.numpy())\n",
    "            all_labels.extend(labels.numpy())\n",
    "    \n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(all_labels, all_predictions, average='weighted')\n",
    "    \n",
    "    return precision, recall, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Loss: 0.1257\n",
      "Epoch [2/10], Loss: 0.1148\n",
      "Epoch [3/10], Loss: 0.1088\n",
      "Epoch [4/10], Loss: 0.1077\n",
      "Epoch [5/10], Loss: 0.0955\n",
      "Epoch [6/10], Loss: 0.0888\n",
      "Epoch [7/10], Loss: 0.1010\n",
      "Epoch [8/10], Loss: 0.1006\n",
      "Epoch [9/10], Loss: 0.0938\n",
      "Epoch [10/10], Loss: 0.0996\n",
      "Precision: 0.3867\n",
      "Recall: 0.1907\n",
      "F1 Score: 0.2438\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/c2k/Projects/spotify-playlist-ai/venv/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "train_model(model, train_loader, criterion, optimizer)\n",
    "\n",
    "# Evaluate the model\n",
    "precision, recall, f1 = evaluate_model(model, test_loader)\n",
    "\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1 Score: {f1:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Playlist Metrics:\n",
      "Playlist\t\tPrecision\tRecall\t\tF1 Score\tSupport\n",
      "Aeon 2 ⏣ ⌬          \t0.0000\t\t0.0000\t\t0.0000\t\t24\n",
      "Ain't Noise Pollution\t0.7826\t\t0.2466\t\t0.3750\t\t73\n",
      "Awake ⏣ ⌬           \t0.0000\t\t0.0000\t\t0.0000\t\t48\n",
      "Bump Beats 🜍        \t0.6471\t\t0.4400\t\t0.5238\t\t50\n",
      "CRANK 🜍🜂            \t0.0000\t\t0.0000\t\t0.0000\t\t22\n",
      "Chill/Solo Guitar   \t0.9032\t\t0.6829\t\t0.7778\t\t41\n",
      "Creekwalk           \t0.7143\t\t0.3226\t\t0.4444\t\t62\n",
      "Critical Path 🜃     \t0.7241\t\t0.4118\t\t0.5250\t\t102\n",
      "Dissolve ⏣ ⌬        \t0.0000\t\t0.0000\t\t0.0000\t\t26\n",
      "Evolution 🜄         \t0.0000\t\t0.0000\t\t0.0000\t\t40\n",
      "Glitch Mind 🜂🜁      \t0.0000\t\t0.0000\t\t0.0000\t\t54\n",
      "Good Life ⏣ ⌬       \t0.0000\t\t0.0000\t\t0.0000\t\t25\n",
      "Horizon ⏣ ⌬         \t0.0000\t\t0.0000\t\t0.0000\t\t25\n",
      "Indie               \t0.0000\t\t0.0000\t\t0.0000\t\t62\n",
      "Keep it smooth 🜩    \t0.0000\t\t0.0000\t\t0.0000\t\t22\n",
      "Overclock 🜁         \t0.6667\t\t0.0909\t\t0.1600\t\t22\n",
      "Reggae              \t0.3333\t\t0.0400\t\t0.0714\t\t25\n",
      "Simp DM 🜄           \t0.0000\t\t0.0000\t\t0.0000\t\t30\n",
      "Vantage ⏣ 🜔 ⌬       \t0.8261\t\t0.5429\t\t0.6552\t\t70\n",
      "WALK UP LIKE 🜂      \t0.6667\t\t0.0645\t\t0.1176\t\t62\n",
      "☉ Pacific           \t0.0000\t\t0.0000\t\t0.0000\t\t79\n",
      "☿gone beyond going☿ \t1.0000\t\t0.7143\t\t0.8333\t\t21\n",
      "🜊 Hoppin            \t0.7273\t\t0.3902\t\t0.5079\t\t41\n",
      "🜻 Chef Up           \t0.0000\t\t0.0000\t\t0.0000\t\t24\n",
      "🝓☉ Alt n shit ☉🝓    \t0.0000\t\t0.0000\t\t0.0000\t\t30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/c2k/Projects/spotify-playlist-ai/venv/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "def print_playlist_metrics(model, test_loader, mlb, threshold=0.5):\n",
    "    model.eval()\n",
    "    all_predictions = []\n",
    "    all_labels = []\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            outputs = model(inputs)\n",
    "            predictions = (outputs >= threshold).float()\n",
    "            all_predictions.extend(predictions.numpy())\n",
    "            all_labels.extend(labels.numpy())\n",
    "    \n",
    "    precision, recall, f1, support = precision_recall_fscore_support(all_labels, all_predictions, average=None)\n",
    "    \n",
    "    print(\"\\nPlaylist Metrics:\")\n",
    "    print(\"Playlist\\t\\tPrecision\\tRecall\\t\\tF1 Score\\tSupport\")\n",
    "    for i, playlist in enumerate(mlb.classes_):\n",
    "        print(f\"{playlist:<20}\\t{precision[i]:.4f}\\t\\t{recall[i]:.4f}\\t\\t{f1[i]:.4f}\\t\\t{support[i]}\")\n",
    "\n",
    "# Print detailed metrics\n",
    "print_playlist_metrics(model, test_loader, mlb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
